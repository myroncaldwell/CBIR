{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-30T08:39:14.773686Z",
     "start_time": "2021-10-30T08:38:47.079770Z"
    }
   },
   "outputs": [],
   "source": [
    "# dataset\n",
    "from torchvision import datasets\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "# visualization\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tabulate import tabulate\n",
    "\n",
    "# preprocessing\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# model\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchvision.models as models\n",
    "\n",
    "# dimensionality reduction\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 1. Load Data & Preprocessing\n",
    "\n",
    "**Dataset**: <a href=\"https://www.cs.toronto.edu/~kriz/cifar.html\" target=\"_blank\">CIFAR-10</a>\n",
    "\n",
    "**Classes**: airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck\n",
    "\n",
    "The following transformations are used:\n",
    "* **resize**\n",
    "* **center crop**\n",
    "* **normalization**: from [0, 255] to [0, 1]\n",
    "* **standardization**: by substracting the mean and dividing with std according to ImageNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-30T08:39:14.789581Z",
     "start_time": "2021-10-30T08:39:14.775583Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def load_data(data_dir):\n",
    "    \"\"\" Create train and test pytorch dataset objects from CIFAR-10.\n",
    "    \n",
    "    The following tranformations are applied on CIFAR-10:\n",
    "        * resize images,\n",
    "        * center crop images,\n",
    "        * normalization: from [0, 255] to [0, 1] by dividing with 255,\n",
    "        * standardization: by substracting the mean and dividing with std according to ImageNet\n",
    "    \n",
    "    Args:\n",
    "        data_dir:\n",
    "            directory where data will be saved, as a string.\n",
    "    \n",
    "    Returns:\n",
    "        train and test dataset, as pytorch dataset objects.\n",
    "    \"\"\"\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(224), \n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    trainset = datasets.CIFAR10(root=data_dir,\n",
    "                                train=True, \n",
    "                                download=True, \n",
    "                                transform=transform)\n",
    "\n",
    "    testset = datasets.CIFAR10(root=data_dir, \n",
    "                               train=False, \n",
    "                               download=True, \n",
    "                               transform=transform)\n",
    "\n",
    "    return trainset, testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-30T08:39:18.223084Z",
     "start_time": "2021-10-30T08:39:14.792090Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\n",
      "Training data:\n",
      "--------------\n",
      "Dataset CIFAR10\n",
      "    Number of datapoints: 50000\n",
      "    Root location: cifar10\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               Resize(size=224, interpolation=bilinear, max_size=None, antialias=None)\n",
      "               CenterCrop(size=(224, 224))\n",
      "               ToTensor()\n",
      "               Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      "           )\n",
      "Test data:\n",
      "--------------\n",
      "Dataset CIFAR10\n",
      "    Number of datapoints: 10000\n",
      "    Root location: cifar10\n",
      "    Split: Test\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               Resize(size=224, interpolation=bilinear, max_size=None, antialias=None)\n",
      "               CenterCrop(size=(224, 224))\n",
      "               ToTensor()\n",
      "               Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      "           )\n"
     ]
    }
   ],
   "source": [
    "trainset, testset = load_data(data_dir='cifar10')\n",
    "\n",
    "print(f'\\nTraining data:\\n--------------\\n{trainset}')\n",
    "print(f'Test data:\\n--------------\\n{testset}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-30T08:39:18.238074Z",
     "start_time": "2021-10-30T08:39:18.226076Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_dataloader = DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(testset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 2. VGG-16\n",
    "\n",
    "We'll use VGG-16. Since CIFAR-10 dataset has less classes than ImageNet, we'll keep all the layers **freezed**, except of the last one, which will be **unfreezed** and updated with the right number of classes.\n",
    "\n",
    "We've already trained a VGG-16 in a previous Jupyter notebook. The best model configurations are:\n",
    "* **learning rate** (lr): 0.001\n",
    "* **weight decay** (l2): 0.001\n",
    "\n",
    "The results of our trained model are: \n",
    "* Best **epoch**: 61\n",
    "* Best **train**: **accuracy**: 89.4%, **avg loss**: 0.317020\n",
    "* Best **test**: **accuracy**: 85.2%, **avg loss**: 0.429216\n",
    "\n",
    "The purpose of VGG-16, is to extract visual embeddings for each image. <br> \n",
    "We define a **visual embedding** as the features of an image, found by a deep-learning model, at the last fully connected layer (prior to a loss layer). <br>\n",
    "In the case of VGG-16, we extract the visual embeddings from the **5th fully connected layer (fc5)**. Thus, the **embeddings vector is of size 4096**. To achieve this, we register a **hook** at this layer which keeps the embeddings each time it processes a batch of data. <br>\n",
    "There are many was to train an embedding layer of a visual deep-learning model. The most common methods make use of the **triplet loss** and the **contrastive loss**. However, for simplicity, we have reduced the problem as a classification problem and we make use of **cross entropy loss**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## 2.1 Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-30T08:39:18.548423Z",
     "start_time": "2021-10-30T08:39:18.240062Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    \"\"\" This helper function sets the .requires_grad attribute of the parameters in the model \n",
    "    to False when we are feature extracting. \n",
    "    \n",
    "    When we are feature extracting and only want to compute gradients for the newly initialized layer, \n",
    "    then we want all of the other parameters to not require gradients.\n",
    "    \n",
    "    Args:\n",
    "        model: \n",
    "            deep learning model, as pytorch object.\n",
    "        feature_extracting:\n",
    "            whether or not we're feature extracting, as boolean.\n",
    "    \"\"\"\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-30T08:39:18.673238Z",
     "start_time": "2021-10-30T08:39:18.550460Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def initialize_model(num_labels, feature_extracting, pretrained=True):\n",
    "    \"\"\" Initialize VGG-16 model and reshape the last layer with the correct number of classes.\n",
    "    \n",
    "    Since VGG-16 has been pretrained on Imagenet, it has output layers of size 1000, one node for each class. \n",
    "    We reshape the last layer to have the same number of inputs as before, and to have the same number of \n",
    "    outputs as the number of classes in our the dataset.\n",
    "    \n",
    "    Args:\n",
    "        num_labels:\n",
    "            number of labels in our dataset, as integer.\n",
    "        feature_extracting:\n",
    "          flag for feature extracting (when False, we finetune the whole model, \n",
    "          when True we only update the reshaped layer params), as boolean.\n",
    "        pretrained:\n",
    "            whether or not we want the pretrained version of AlexNet, as boolean.\n",
    "    \n",
    "    Returns:\n",
    "        VGG-16 model, as pytorch object\n",
    "    \"\"\"\n",
    "    model = models.vgg16(pretrained=pretrained)\n",
    "    \n",
    "    set_parameter_requires_grad(model, feature_extracting)\n",
    "    \n",
    "    last_layer_in_ftrs = model.classifier[6].in_features\n",
    "    model.classifier[6] = nn.Linear(last_layer_in_ftrs, num_labels)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## 2.2 Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-30T08:39:19.988075Z",
     "start_time": "2021-10-30T08:39:18.675052Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(f\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Using {device} device')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-30T08:39:39.407925Z",
     "start_time": "2021-10-30T08:39:19.989831Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model architecture:\n",
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): ReLU(inplace=True)\n",
      "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (27): ReLU(inplace=True)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=4096, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "num_labels = 10\n",
    "feature_extracting = True\n",
    "pretrained = True\n",
    "\n",
    "model = initialize_model(num_labels=num_labels, \n",
    "                         feature_extracting=feature_extracting, \n",
    "                         pretrained=pretrained).to(device)\n",
    "\n",
    "print(f'Model architecture:\\n{model}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-30T08:39:44.599920Z",
     "start_time": "2021-10-30T08:39:39.411104Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('../saved-model/vgg16-weights.pth', map_location=\"cuda:0\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## 2.3 Feature Extraction -- Register Hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-30T08:39:44.615855Z",
     "start_time": "2021-10-30T08:39:44.603010Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.hooks.RemovableHandle at 0x17ce9000b08>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = []\n",
    "\n",
    "def get_features():\n",
    "    def hook(model, input, output):\n",
    "        global features\n",
    "        features.append(output.detach().cpu().numpy())\n",
    "    return hook\n",
    "\n",
    "model.classifier[5].register_forward_hook(get_features())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## 2.4 Feature Extraction -- Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-30T08:39:44.819815Z",
     "start_time": "2021-10-30T08:39:44.619226Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def predict(dataloader, model, device):\n",
    "    \"\"\" Predict with deep-learning model.\n",
    "\n",
    "    Args:\n",
    "        dataloader:\n",
    "            pytorch DataLoader object.\n",
    "        model:\n",
    "            deep learning model, as pytorch object.\n",
    "        device:\n",
    "            device where the deep-learning model will run (cpu, gpu), as string.\n",
    "\n",
    "    Returns:\n",
    "         predictions, as a list of integers.\n",
    "    \"\"\"\n",
    "    pred_concat = []\n",
    "    \n",
    "    model.eval()  # put on evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for X,_ in dataloader:\n",
    "            X = X.to(device)\n",
    "\n",
    "            # predict class label\n",
    "            pred = model(X)\n",
    "\n",
    "            # get predicted class label index\n",
    "            for label in pred.argmax(1):    \n",
    "                pred_concat.append(label.item())\n",
    "\n",
    "    return pred_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-30T08:51:45.454910Z",
     "start_time": "2021-10-30T08:39:44.822060Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lampr\\anaconda3\\envs\\cbir-dl\\lib\\site-packages\\torch\\nn\\functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ..\\c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train images: 50000\n",
      "Number of features per train image: 4096\n"
     ]
    }
   ],
   "source": [
    "train_pred = predict(train_dataloader, model, device)\n",
    "\n",
    "train_features = np.vstack(features)\n",
    "print(f'Number of train images: {train_features.shape[0]}\\nNumber of features per train image: {train_features.shape[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-30T08:51:45.532444Z",
     "start_time": "2021-10-30T08:51:45.458026Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# reset features list\n",
    "features = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-30T08:54:08.803166Z",
     "start_time": "2021-10-30T08:51:45.534408Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of test images: 10000\n",
      "Number of features per test image: 4096\n"
     ]
    }
   ],
   "source": [
    "test_pred = predict(test_dataloader, model, device)\n",
    "\n",
    "test_features = np.vstack(features)\n",
    "print(f'Number of test images: {test_features.shape[0]}\\nNumber of features per test image: {test_features.shape[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 3. Principal Component Analysis\n",
    "\n",
    "The idea in this project is to index the visual embeddings of images in Elasticsearch, so that we can retrieve similar images, with respect to a query image.\n",
    "\n",
    "However, **Elasticsearch is able to handle only vectors of size 2048**. So, we need to reduce the dimensions of the visual embeddings found by VGG-16, which are of size 4096.\n",
    "\n",
    "To achieve this, we'll use **dimensionality reduction** techniques. Specifically, we'll make use of **Principal Component Analysis (PCA)** algorithm to reduce the dimensions of the visual embeddings to a **vector of size 2000**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## 3.1 Define PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-30T08:54:08.815653Z",
     "start_time": "2021-10-30T08:54:08.805650Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "n_components = 2000\n",
    "\n",
    "pca = PCA(n_components=n_components)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## 3.2 Train PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-30T08:55:35.186888Z",
     "start_time": "2021-10-30T08:54:08.818614Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(n_components=2000)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.fit(train_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## 3.3 Transform with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-30T08:55:39.796194Z",
     "start_time": "2021-10-30T08:55:35.198846Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reduced features per train image: 2000\n"
     ]
    }
   ],
   "source": [
    "train_reduced_ftrs = pca.transform(train_features)\n",
    "\n",
    "print(f'Number of reduced features per train image: {train_reduced_ftrs.shape[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-30T08:55:40.893202Z",
     "start_time": "2021-10-30T08:55:39.799143Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reduced features per test image: 2000\n"
     ]
    }
   ],
   "source": [
    "test_reduced_ftrs = pca.transform(test_features)\n",
    "\n",
    "print(f'Number of reduced features per test image: {test_reduced_ftrs.shape[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## 3.4 Save "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-30T09:10:21.663242Z",
     "start_time": "2021-10-30T09:10:21.477972Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../saved-model/pca.joblib']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(pca, '../saved-model/pca.joblib')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cbir-dl]",
   "language": "python",
   "name": "conda-env-cbir-dl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "291.2px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
